#ì‹¤í–‰ streamlit run Chatbot_deepseek.py
#ì‹¤í–‰ streamlit run Chatbot_deepseek.py --server.address=0.0.0.0 --server.port=8501

import streamlit as st
import os

from langchain_ollama import ChatOllama
import ollama

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup as bs
from selenium.webdriver.chrome.options import Options
from webdriver_manager.core.os_manager import ChromeType
from selenium.webdriver.common.by import By

from bs4 import BeautifulSoup

import time

#langchain, genai
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import LLMChain
from langchain.prompts import ChatPromptTemplate
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.memory import ConversationBufferWindowMemory

from langgraph.graph import START,END,StateGraph
from typing_extensions import TypedDict,List
from langchain_core.prompts.chat import ChatPromptTemplate
from langchain_core.messages import HumanMessage


from PIL import Image
import base64
import io
from io import BytesIO

class State(TypedDict):
    question: str
    answer: str
    encoded_image : str
    image_format : str

def think_answer_chunk2(state:State):

    query = state['question']
    encoded_image = state['encoded_image']
    format = state['image_format']

    llm = ChatOllama(model=st.session_state.llm_model,
                     temperature=st.session_state['temp_0'])
    
    chain = prompt_func | llm

    # streamìœ¼ë¡œ ì¶œë ¥í•˜ê¸°(https://wikidocs.net/232694)
    think_message_placeholder = st.empty() # DeltaGenerator ë°˜í™˜
    full_response = '' 
    for chunk in chain.stream({'question': query,
                            'image': encoded_image,
                            'image_format': format}):
            
            full_response += chunk.content
            think_message_placeholder.markdown(full_response)

    return {'answer': full_response}

def image_upload():
    
    # íŒŒì¼ ì—…ë¡œë” ìœ„ì ¯
    uploaded_file = st.file_uploader("Choose an image...", type=["jpg","jpeg","png"])

    # íŒŒì¼ì´ ì—…ë¡œë“œë˜ë©´ ì²˜ë¦¬
    if uploaded_file is not None:

        # íŒŒì¼ì„ ë°”ì´ë„ˆë¦¬ ë°ì´í„°ë¡œ ì½ê¸°
        img_bytes = uploaded_file.read()

        # PILë¡œ ì´ë¯¸ì§€ë¥¼ ì—´ê³  ìŠ¤íŠ¸ë¦¼ë¦¿ ì•±ì— í‘œì‹œ
        image = Image.open(uploaded_file)
        st.image(image, caption="Uploaded Image.")
        st.write("Image uploaded successfully!")
    else:
        st.write("Please upload a JPG or PNG file.")

    buffered = BytesIO()
    format = image.format.lower()  # Get the file format (e.g. "jpg", "jpeg", "png")
    image.save(buffered, format=format)
    img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")

    return img_str,format

def prompt_func(data):
    text = data['question']
    image = data['image']
    format = data['image_format']

    image_part = {
        "type": "image_url",
        "image_url": f"data:image/{format};base64,{image}",
    }

    content_parts = []

    text_part = {
        "type": "text",
        "text": text
    }

    content_parts.append(image_part)
    content_parts.append(text_part)

    return [HumanMessage(content=content_parts)]

def langgraph_build():
    builder = StateGraph(State) # ê·¸ë˜í”„ ìƒì„±
    
    #step3. node ì¶”ê°€í•˜ê¸°
    builder.add_node("think_answer", think_answer_chunk2)
    
    #step4. edge ì •ì˜í•˜ê¸°
    builder.add_edge(START, "think_answer")
    builder.add_edge("think_answer",END)

    #step5. graph compileí•˜ê¸°
    graph = builder.compile()

    return graph

def main():

    #os.environ['KMP_DUPLICATE_LIB_OK']='True'   # ì¶©ëŒ ì œì–´ ì£¼ì˜ : ì„±ëŠ¥ ì €í•˜/ì˜¤ë¥˜ ê°€ëŠ¥ì„± ìˆìŒ

    #st.set_page_config(page_title="Lagnchain_with_pdf", page_icon=":books:")

    st.title("ğŸ’¬ Chatbot_exaone-deep")
    st.caption("ğŸš€ A Streamlit chatbot powered by exaone-deep")

    #1. st.session_state ì´ˆê¸°í™”
    if "messages_0" not in st.session_state:
        st.session_state['messages_0'] = []
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "memory_0" not in st.session_state:
        #ìœˆë„ìš° í¬ê¸° kë¥¼ ì§€ì •í•˜ë©´ ìµœê·¼ kê°œì˜ ëŒ€í™”ë§Œ ê¸°ì–µí•˜ê³  ì´ì „ ëŒ€í™”ëŠ” ì‚­ì œ
        st.session_state.memory_0 = ConversationBufferWindowMemory(memory_key="chat_history",k=4)
    if "temp_0" not in st.session_state:
        st.session_state.temp_0 = 0

    if "translate_model" not in st.session_state:
        st.session_state.translate_model = ''

    if "llm_model" not in st.session_state:
        st.session_state.llm_model = ''

    if "messages_img" not in st.session_state:
        st.session_state['messages_img'] = [] 
        #st.session_stateì— messagesê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”

    if "chat_img" not in st.session_state:
        st.session_state.chat_img = ''

    if "image_format" not in st.session_state:
        st.session_state.image_format = ''
        
    with st.sidebar:

        model_name = []

        # ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        models = ollama.list()

        # ëª¨ë¸ ëª©ë¡ ì¶œë ¥í•˜ê¸°
        for model in models['models']:
            #st.write(model['model'])
            model_name.append(model['model'])

        st.session_state.llm_model = st.sidebar.selectbox('Choose LLM Model', 
                                                        model_name,key='selected_model')
        
        st.info(f'{st.session_state.llm_model}ì„ ì„ íƒí•˜ì…¨ìŠµë‹ˆë‹¤.')

        if data_clear :=st.button("ëŒ€í™” í´ë¦¬ì–´"):
            st.session_state['messages_0'] = [] #st.session_state[messages]ë¥¼ ì´ˆê¸°í™”
            st.session_state.chat_history = []
            st.session_state['messages_img'] = []
            st.session_state.chat_img = ''
            st.session_state.image_format = ''

        st.session_state['temp_0'] = st.slider("llm ìƒì„±ì‹œ ì°½ì˜ì„±ì„ ì¡°ì ˆí•©ë‹ˆë‹¤. 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì°½ì˜ì ì¸ ì¶œë ¥ì´ ìƒì„±ë©ë‹ˆë‹¤", 0.0, 1.0, 0.0)

        st.info(f"ì„ íƒëœ llm ì°½ì˜ì„±ì€ {st.session_state['temp_0']} ì…ë‹ˆë‹¤.")

        st.session_state['translate_model'] = st.radio("Translate Model ì„ íƒ", ('google_translate', 'deepl_translate'), key="model_0")

        st.info(f"ì„ íƒëœ ë²ˆì—­ ëª¨ë¸ì€ {st.session_state['translate_model']} ì…ë‹ˆë‹¤.")

    #2. graph build
    graph = langgraph_build()

    # ì´ë¯¸ì§€ ë¡œë“œ
    st.session_state.chat_img, st.session_state.image_format = image_upload()

    st.chat_message("assistant").write("ì•ˆë…•í•˜ì„¸ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")

    #2. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì¶œë ¥
    # st.session_state['chat_history']ê°€ ìˆìœ¼ë©´ ì‹¤í–‰
    if ("chat_history" in st.session_state) and (len(st.session_state['chat_history'])>0):
        #st.session_state['messages']ëŠ” tuple í˜•íƒœë¡œ ì €ì¥ë˜ì–´ ìˆìŒ.
        for role, message in st.session_state['chat_history']: 
            st.chat_message(role).write(message)

    #3. queryë¥¼ ì…ë ¥ë°›ëŠ”ë‹¤.
    if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."): 

        #4.'user' iconìœ¼ë¡œ queryë¥¼ ì¶œë ¥í•œë‹¤.
        st.chat_message("user").write(query)

        #5. queryë¥¼ session_state 'user'ì— append í•œë‹¤.
        st.session_state['chat_history'].append(('user',query))

        with st.chat_message("assistant"):

            with st.spinner("Thinking..."):
                # chain í˜¸ì¶œ
                start_time = time.time()
                response = graph.invoke({'question':query,
                                         'encoded_image':st.session_state.chat_img,
                                         'image_format':st.session_state.image_format})

                end_time = time.time()
                total_time = (end_time - start_time)
                st.info(f"ê²€ìƒ‰ ì†Œìš” ì‹œê°„: {total_time}ì´ˆ")
                st.session_state['chat_history'].append(('assistant',response['answer']))

if __name__ == '__main__':
    main()