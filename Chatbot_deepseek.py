#ì‹¤í–‰ streamlit run Chatbot_deepseek.py
#ì‹¤í–‰ streamlit run Chatbot_deepseek.py --server.address=0.0.0.0 --server.port=8501

import streamlit as st
import os

from langchain_ollama import ChatOllama
import ollama

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup as bs
from selenium.webdriver.chrome.options import Options
from webdriver_manager.core.os_manager import ChromeType
from selenium.webdriver.common.by import By

from bs4 import BeautifulSoup

import time

#langchain, genai
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import LLMChain
from langchain.prompts import ChatPromptTemplate
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.memory import ConversationBufferWindowMemory

from langgraph.graph import START,END,StateGraph
from typing_extensions import TypedDict,List
from langchain_core.prompts.chat import ChatPromptTemplate

def get_driver():


    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')

    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML,   #like Gecko) Chrome/58.0.3029.110 Safari/537.3')

    # WebDriver ì„¤ì •
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)

    return driver

def google_translate(query: str) -> str:
    """êµ¬ê¸€ ë²ˆì—­(í•œêµ­ì–´->ì˜ì–´ ë²ˆì—­)"""

    driver = get_driver()

    #headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'}
    search_url = f"https://translate.google.com/?sl=ko&tl=en&text={query}&op=translate"

    driver.get(search_url)

    time.sleep(3)

    result = driver.find_elements(By.CLASS_NAME, 'ryNqvb')[0].text

    return result

def deepl_translate(query: str) -> str:
    """deepl ë²ˆì—­(í•œêµ­ì–´->ì˜ì–´ ë²ˆì—­)"""

    driver = get_driver()

    #headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'}
    search_url = f"https://www.deepl.com/ko/translator-ppc?utm_source=naver&utm_medium=paid&utm_campaign=KO_Naver_Brand&utm_adgroup=KO_Naver_Brand&utm_term=DEEPL&n_media=27758&n_query=DEEPL&n_rank=1&n_ad_group=grp-a001-04-000000045475249&n_ad=nad-a001-04-000000335393648&n_keyword_id=nkw-a001-04-000006578576419&n_keyword=DEEPL&n_campaign_type=4&n_contract=tct-a001-04-000000001044608&n_ad_group_type=5&NaPm=ct%3Dm7gzgcuw%7Cci%3D0zq0001ZupDBaYysA1mW%7Ctr%3Dbrnd%7Chk%3D45533394f7471da3957334fe15af5fd82d683fdc%7Cnacn%3DaoNiBcQ568tr#ko/en-us/{query}"

    driver.get(search_url)

    time.sleep(3)

    html = driver.page_source

    soup = BeautifulSoup(html, 'html.parser')

    results = soup.find_all('span', class_='--l --r sentence_highlight')

    result = results[1].text

    return result

class State(TypedDict):
    question: str
    answer: str

def transfer_ko2en2(state:State):
    question_co = state['question'] # state['question'] -> question_co 
    if st.session_state.translate_model == 'google_translate': 
        question_en = google_translate(question_co)           #ì˜ì–´ë¡œ ë²ˆì—­
    elif st.session_state.translate_model == 'deepl_translate':
        question_en = deepl_translate(question_co)            #ì˜ì–´ë¡œ ë²ˆì—­
    else:
        st.error('Translate Model Error')
        st.stop()
    st.write(question_en)                                     #ì˜ì–´ question ì¶œë ¥
    return {'question': question_en}                          #stat

def think_answer(state:State):

    query = state['question'] #state['question'] -> query
    deepseek = ChatOllama(model='deepseek-r1:14b',temperature=st.session_state['temp_0'])
    response = deepseek.invoke(query) #query -> response
    st.write(response.content)
    return {'answer': response.content}


def think_answer_chunk(state:State):

    query = state['question'] #state['question'] -> query
    deepseek = ChatOllama(model='deepseek-r1:14b',temperature=st.session_state['temp_0'])
    response = ''
    sentence = ''
    for chunk in deepseek.stream(query):
        if chunk.content in ['\n\n', '\n\n\n']:
            st.write(sentence)
            sentence = ''
        else:
            sentence = sentence + chunk.content
        
        response = response + chunk.content

    return {'answer': response}


def think_answer_chunk2(state:State):

    query = state['question'] #state['question'] -> query
    deepseek = ChatOllama(model='deepseek-r1:14b',temperature=st.session_state['temp_0'])
    # streamìœ¼ë¡œ ì¶œë ¥í•˜ê¸°(https://wikidocs.net/232694)
    think_message_placeholder = st.empty() # DeltaGenerator ë°˜í™˜
    full_response = '' 
    for chunk in deepseek.stream(query):
            full_response += chunk.content
            think_message_placeholder.markdown(full_response)

    #st.write(full_response)    

    return {'answer': full_response}

def transfer_en2ko(state:State):
    
    answer_en = state['answer'] # state['answer'] -> question_en
    transfer_prompt_en2ko = ChatPromptTemplate([
        (
            "system",
            """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ Queryë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤.
            ê·œì¹™ 1: Queryì˜ ë‹µë³€ì€ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
            ê·œì¹™ 2: Query ë²ˆì—­ ê²°ê³¼ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.
            """
        ),
        (
            "human",
            """
            Query: {question}
            """
        )]) 
    messages = transfer_prompt_en2ko.invoke({'question':answer_en})
    exaone = ChatOllama(model='exaone3.5:7.8b',temperature=0) 
    answer_ko = exaone.invoke(messages)               #í•œêµ­ì–´ë¡œ ë²ˆì—­ í›„ answer_ko
    #print(answer_ko.content)                         #ì˜ì–´ answer_ko ì¶œë ¥
    return {'answer': answer_ko.content}             #state['answer']=answer_ko

def transfer_en2ko_chunk(state:State):
    
    answer_en = state['answer'] # state['answer'] -> question_en
    transfer_prompt_en2ko = ChatPromptTemplate([
        (
            "system",
            """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ Queryë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤.
            ê·œì¹™ 1: Queryì˜ ë‹µë³€ì€ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
            ê·œì¹™ 2: Query ë²ˆì—­ ê²°ê³¼ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.
            """
        ),
        (
            "human",
            """
            Query: {question}
            """
        )]) 
    messages = transfer_prompt_en2ko.invoke({'question':answer_en})
    exaone = ChatOllama(model='exaone3.5:7.8b',temperature=0)
    response = ''
    sentence = ''
    for chunk in exaone.stream(messages):
        if chunk.content in ['\n\n', '\n\n\n']:
            st.write(sentence)
            sentence = ''
        else:
            sentence = sentence + chunk.content

        response = response + chunk.content

    st.write(response)
        
    return {'answer': response}             #state['answer']=answer_ko

def transfer_en2ko_chunk2(state:State):
    
    answer_en = state['answer'] # state['answer'] -> question_en
    transfer_prompt_en2ko = ChatPromptTemplate([
        (
            "system",
            """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ Queryë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤.
            ê·œì¹™ 1: Queryì˜ ë‹µë³€ì€ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
            ê·œì¹™ 2: Query ë²ˆì—­ ê²°ê³¼ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.
            """
        ),
        (
            "human",
            """
            Query: {question}
            """
        )]) 
    messages = transfer_prompt_en2ko.invoke({'question':answer_en})
    exaone = ChatOllama(model='exaone3.5:7.8b',temperature=0)

    transfer_en2ko_message_placeholder = st.empty() # DeltaGenerator ë°˜í™˜
    full_response = '' 
    for chunk in exaone.stream(messages):
            full_response += chunk.content
            transfer_en2ko_message_placeholder.markdown(full_response)

    #st.write(full_response)

    return {'answer': full_response}             #state['answer']=full_response


def langgraph_build():
    builder = StateGraph(State) # ê·¸ë˜í”„ ìƒì„±
    
    #step3. node ì¶”ê°€í•˜ê¸°
    builder.add_node("transfer_ko2en", transfer_ko2en2)
    builder.add_node("transfer_en2ko", transfer_en2ko_chunk2)
    builder.add_node("think_answer", think_answer_chunk2)
    
    #step4. edge ì •ì˜í•˜ê¸°
    builder.add_edge(START, "transfer_ko2en")
    builder.add_edge("transfer_ko2en", "think_answer")
    builder.add_edge("think_answer", "transfer_en2ko")
    builder.add_edge("transfer_en2ko",END)

    #step5. graph compileí•˜ê¸°
    graph = builder.compile()

    return graph

def main():

    #os.environ['KMP_DUPLICATE_LIB_OK']='True'   # ì¶©ëŒ ì œì–´ ì£¼ì˜ : ì„±ëŠ¥ ì €í•˜/ì˜¤ë¥˜ ê°€ëŠ¥ì„± ìˆìŒ

    #st.set_page_config(page_title="Lagnchain_with_pdf", page_icon=":books:")

    st.title("ğŸ’¬ Chatbot_deepseek")
    st.caption("ğŸš€ A Streamlit chatbot powered by deepseek")

    #1. st.session_state ì´ˆê¸°í™”
    if "messages_0" not in st.session_state:
        st.session_state['messages_0'] = []
    if "chat_history_0" not in st.session_state:
        st.session_state.chat_history_0 = []
    if "memory_0" not in st.session_state:
        #ìœˆë„ìš° í¬ê¸° kë¥¼ ì§€ì •í•˜ë©´ ìµœê·¼ kê°œì˜ ëŒ€í™”ë§Œ ê¸°ì–µí•˜ê³  ì´ì „ ëŒ€í™”ëŠ” ì‚­ì œ
        st.session_state.memory_0 = ConversationBufferWindowMemory(memory_key="chat_history",
                                                                   k=4)
    if "temp_0" not in st.session_state:
        st.session_state.temp_0 = 0

    if "translate_model" not in st.session_state:
        st.session_state.translate_model = ''
    
    with st.sidebar:

        if data_clear :=st.button("ëŒ€í™” í´ë¦¬ì–´"):
            st.session_state['messages_0'] = [] #st.session_state[messages]ë¥¼ ì´ˆê¸°í™”
            st.session_state.chat_history_0 = []

        st.session_state['temp_0'] = st.slider("llm ìƒì„±ì‹œ ì°½ì˜ì„±ì„ ì¡°ì ˆí•©ë‹ˆë‹¤. 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì°½ì˜ì ì¸ ì¶œë ¥ì´ ìƒì„±ë©ë‹ˆë‹¤", 0.0, 1.0, 0.0)

        st.info(f"ì„ íƒëœ llm ì°½ì˜ì„±ì€ {st.session_state['temp_0']} ì…ë‹ˆë‹¤.")

        st.session_state['translate_model'] = st.radio("Translate Model ì„ íƒ", ('google_translate', 'deepl_translate'), key="model_0")

        st.info(f"ì„ íƒëœ ë²ˆì—­ ëª¨ë¸ì€ {st.session_state['translate_model']} ì…ë‹ˆë‹¤.")


    st.chat_message("assistant").write("ì•ˆë…•í•˜ì„¸ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")

    graph = langgraph_build()

    #3. queryë¥¼ ì…ë ¥ë°›ëŠ”ë‹¤.
    if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."): 

        #4.'user' iconìœ¼ë¡œ queryë¥¼ ì¶œë ¥í•œë‹¤.
        st.chat_message("user").write(query)

        #5. queryë¥¼ session_state 'user'ì— append í•œë‹¤.
        st.session_state['messages_0'].append(('user',query))
        
        with st.chat_message("assistant"):

            with st.spinner("Thinking..."):
                # chain í˜¸ì¶œ
                start_time = time.time()
                response = graph.invoke({'question':query})
                #st.write(response['answer'])
                end_time = time.time()
                total_time = (end_time - start_time)
                st.info(f"ê²€ìƒ‰ ì†Œìš” ì‹œê°„: {total_time}ì´ˆ")
                st.session_state['messages_0'].append(('assistant',response['answer']))

if __name__ == '__main__':
    main()